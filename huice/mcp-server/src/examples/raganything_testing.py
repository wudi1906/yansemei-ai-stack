"""
Copyright (c) 2025 Dean Wu. All rights reserved.
AuroraAI Project.
"""

import asyncio
# pylint: disable  MC80OmFIVnBZMlhsa0xUb3Y2bzZZbUpxUWc9PTpkZTg0M2U0YQ==

from lightrag.llm.ollama import ollama_embed
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # 设置 API 配置
    api_key = "sk-e995a95f08e14ff39904d41bbf54e742"
    base_url = "https://api.deepseek.com/v1"  # 可选

    vm_api_key = "90b19007-6c1e-4123-9777-a54d6b3a258a"
    vm_base_url = "https://ark.cn-beijing.volces.com/api/v3"

    # 创建 RAGAnything 配置
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="docling",  # 选择解析器：mineru 或 docling
        parse_method="auto",  # 解析方法：auto, ocr 或 txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )
# fmt: off  MS80OmFIVnBZMlhsa0xUb3Y2bzZZbUpxUWc9PTpkZTg0M2U0YQ==

    # 定义 LLM 模型函数
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "deepseek-chat",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # 定义视觉模型函数用于图像处理
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # 如果提供了messages格式（用于多模态VLM增强查询），直接使用
        if messages:
            return openai_complete_if_cache(
                "doubao-seed-1-6-251015",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=vm_api_key,
                base_url=vm_base_url,
                **kwargs,
            )
        # 传统单图片格式
        elif image_data:
            return openai_complete_if_cache(
                "doubao-seed-1-6-251015",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=vm_api_key,
                base_url=vm_base_url,
                **kwargs,
            )
        # 纯文本格式
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
# fmt: off  Mi80OmFIVnBZMlhsa0xUb3Y2bzZZbUpxUWc9PTpkZTg0M2U0YQ==

    # 定义嵌入函数
    embedding_func = EmbeddingFunc(
        embedding_dim=1024,
        max_token_size=32000,
        func=lambda texts: ollama_embed(
            texts,
            embed_model="qwen3-embedding:0.6b",
            api_key="sk-danwen",
            host="http://140.82.49.72:11434",
        )
    )

    # 初始化 RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # 处理文档
    await rag.process_document_complete(
        file_path="llm_course.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # 查询处理后的内容
    # 纯文本查询 - 基本知识库搜索
    text_result = await rag.aquery(
        "文档的主要内容是什么？",
        mode="hybrid"
    )
    print("文本查询结果:", text_result)
# pylint: disable  My80OmFIVnBZMlhsa0xUb3Y2bzZZbUpxUWc9PTpkZTg0M2U0YQ==

    # 多模态查询 - 包含具体多模态内容的查询
    multimodal_result = await rag.aquery_with_multimodal(
        "分析这个性能数据并解释与现有文档内容的关系",
        multimodal_content=[{
            "type": "table",
            "table_data": """系统,准确率,F1分数
                            RAGAnything,95.2%,0.94
                            基准方法,87.3%,0.85""",
            "table_caption": "性能对比结果"
        }],
        mode="hybrid"
    )
    print("多模态查询结果:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())