"""
RAG Anything æ¡†æ¶çš„ MCP æœåŠ¡å™¨

æœ¬æ¨¡å—ä¸º RAG Anything æ¡†æ¶æä¾›æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æœåŠ¡å™¨å®ç°ï¼Œ
é€šè¿‡ MCP å·¥å…·å’Œèµ„æºæš´éœ²å…¶å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†å’ŒæŸ¥è¯¢åŠŸèƒ½ã€‚
"""
"""
Copyright (c) 2025 Dean Wu. All rights reserved.
AuroraAI Project.
"""


import argparse
import os
import json
import logging
from contextlib import asynccontextmanager
from typing import Any, AsyncIterator, Optional, List, Dict
from dotenv import load_dotenv
from pathlib import Path

from fastmcp import FastMCP, Context
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.llm.ollama import ollama_embed
from lightrag.utils import EmbeddingFunc, logger


class RAGAnythingConnector:
    """
    RAG Anything æ¡†æ¶çš„è¿æ¥å™¨ç±»

    ç®¡ç† RAGAnything å®ä¾‹çš„ç”Ÿå‘½å‘¨æœŸï¼Œå¹¶ä¸º MCP å·¥å…·æä¾›æ¸…æ™°çš„æ¥å£
    ä»¥ä¾¿ä¸æ¡†æ¶è¿›è¡Œäº¤äº’ã€‚
    """

    def __init__(
        self,
        working_dir: str,
        llm_api_key: Optional[str] = None,
        llm_base_url: Optional[str] = None,
        llm_model: str = "deepseek-chat",
        vision_api_key: Optional[str] = None,
        vision_base_url: Optional[str] = None,
        vision_model: Optional[str] = None,
        embedding_host: Optional[str] = None,
        embedding_model: str = "qwen3-embedding:0.6b",
        embedding_dim: int = 1024,
        use_ollama_embedding: bool = True,
        parser: str = "docling",
        parse_method: str = "auto",
        enable_image_processing: bool = True,
        enable_table_processing: bool = True,
        enable_equation_processing: bool = True,
        load_existing: bool = True,
    ):
        """
        åˆå§‹åŒ– RAG Anything è¿æ¥å™¨

        å‚æ•°:
            working_dir: RAG å­˜å‚¨ç›®å½•
            llm_api_key: LLM çš„ API å¯†é’¥
            llm_base_url: LLM API çš„åŸºç¡€ URL
            llm_model: LLM æ¨¡å‹åç§°ï¼ˆé»˜è®¤: deepseek-chatï¼‰
            vision_api_key: è§†è§‰æ¨¡å‹çš„ API å¯†é’¥
            vision_base_url: è§†è§‰ API çš„åŸºç¡€ URL
            vision_model: è§†è§‰æ¨¡å‹åç§°
            embedding_host: Ollama åµŒå…¥æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤: http://35.235.113.151:11434ï¼‰
            embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen3-embedding:0.6bï¼‰
            embedding_dim: åµŒå…¥ç»´åº¦ï¼ˆé»˜è®¤: 1024ï¼‰
            use_ollama_embedding: æ˜¯å¦ä½¿ç”¨ Ollama åµŒå…¥ï¼ˆé»˜è®¤: Trueï¼‰
            parser: ä½¿ç”¨çš„è§£æå™¨ (mineru æˆ– docling)ï¼ˆé»˜è®¤: doclingï¼‰
            parse_method: è§£ææ–¹æ³• (auto, ocr, txt)ï¼ˆé»˜è®¤: autoï¼‰
            enable_image_processing: å¯ç”¨å›¾åƒå¤„ç†
            enable_table_processing: å¯ç”¨è¡¨æ ¼å¤„ç†
            enable_equation_processing: å¯ç”¨å…¬å¼å¤„ç†
            load_existing: æ˜¯å¦åŠ è½½å·²å­˜åœ¨çš„çŸ¥è¯†åº“ï¼ˆé»˜è®¤: Trueï¼‰
        """
        self.working_dir = working_dir
        self.llm_api_key = llm_api_key
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.vision_api_key = vision_api_key or llm_api_key
        self.vision_base_url = vision_base_url or llm_base_url
        self.vision_model = vision_model
        self.embedding_host = embedding_host or "http://35.235.113.151:11434"
        self.embedding_model = embedding_model
        self.embedding_dim = embedding_dim
        self.use_ollama_embedding = use_ollama_embedding
        self.load_existing = load_existing

        # åˆ›å»ºé…ç½®
        self.config = RAGAnythingConfig(
            working_dir=working_dir,
            parser=parser,
            parse_method=parse_method,
            enable_image_processing=enable_image_processing,
            enable_table_processing=enable_table_processing,
            enable_equation_processing=enable_equation_processing,
            display_content_stats=True,
        )

        # åˆ›å»ºæ¨¡å‹å‡½æ•°
        self.llm_model_func = self._create_llm_func()
        self.vision_model_func = self._create_vision_func() if vision_model else None
        self.embedding_func = self._create_embedding_func()

        # RAGAnything å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.rag: Optional[RAGAnything] = None
        self.lightrag_instance: Optional[LightRAG] = None
        self._initialized = False

    async def initialize(self):
        """
        åˆå§‹åŒ– RAGAnything å®ä¾‹

        å¦‚æœ load_existing=True ä¸”å­˜åœ¨å·²æœ‰çŸ¥è¯†åº“ï¼Œåˆ™åŠ è½½å·²æœ‰çš„ LightRAG å®ä¾‹ï¼›
        å¦åˆ™åˆ›å»ºæ–°çš„ RAGAnything å®ä¾‹ã€‚
        """
        if self._initialized:
            logger.info("RAGAnything å·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡")
            return

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²æœ‰çš„çŸ¥è¯†åº“
        if self.load_existing and os.path.exists(self.working_dir) and os.listdir(self.working_dir):
            logger.info(f"âœ… å‘ç°å·²å­˜åœ¨çš„çŸ¥è¯†åº“: {self.working_dir}")
            logger.info("æ­£åœ¨åŠ è½½å·²æœ‰çŸ¥è¯†åº“...")

            # åˆ›å»º LightRAG å®ä¾‹å¹¶åŠ è½½å·²æœ‰æ•°æ®
            self.lightrag_instance = LightRAG(
                working_dir=self.working_dir,
                llm_model_func=self.llm_model_func,
                embedding_func=self.embedding_func,
            )

            # åˆå§‹åŒ–å­˜å‚¨ï¼ˆåŠ è½½å·²æœ‰æ•°æ®ï¼‰
            await self.lightrag_instance.initialize_storages()
            await initialize_pipeline_status()
            logger.info("âœ… å·²æœ‰çŸ¥è¯†åº“åŠ è½½å®Œæˆ")

            # ä½¿ç”¨å·²æœ‰çš„ LightRAG å®ä¾‹åˆ›å»º RAGAnything
            self.rag = RAGAnything(
                lightrag=self.lightrag_instance,
                vision_model_func=self.vision_model_func,
            )
            logger.info("âœ… RAGAnything å·²ä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“åˆå§‹åŒ–")
        else:
            # åˆ›å»ºæ–°çš„ RAGAnything å®ä¾‹
            if not os.path.exists(self.working_dir):
                logger.info(f"åˆ›å»ºæ–°çš„å·¥ä½œç›®å½•: {self.working_dir}")
                os.makedirs(self.working_dir, exist_ok=True)

            logger.info("æ­£åœ¨åˆ›å»ºæ–°çš„ RAGAnything å®ä¾‹...")
            self.rag = RAGAnything(
                config=self.config,
                llm_model_func=self.llm_model_func,
                vision_model_func=self.vision_model_func,
                embedding_func=self.embedding_func,
            )
            logger.info("âœ… RAGAnything æ–°å®ä¾‹åˆ›å»ºå®Œæˆ")

        self._initialized = True
# noqa  MC80OmFIVnBZMlhsa0xUb3Y2bzZNalJxY1E9PTo0Nzk0MDQyYQ==

    def _create_llm_func(self):
        """åˆ›å»º LLM æ¨¡å‹å‡½æ•°"""
        def llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            return openai_complete_if_cache(
                self.llm_model,
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=self.llm_api_key,
                base_url=self.llm_base_url,
                **kwargs,
            )
        return llm_func

    def _create_vision_func(self):
        """åˆ›å»ºè§†è§‰æ¨¡å‹å‡½æ•°"""
        def vision_func(
            prompt,
            system_prompt=None,
            history_messages=[],
            image_data=None,
            messages=None,
            **kwargs
        ):
            # å¤„ç†æ¶ˆæ¯æ ¼å¼ï¼ˆç”¨äº VLM å¢å¼ºæŸ¥è¯¢ï¼‰
            if messages:
                return openai_complete_if_cache(
                    self.vision_model,
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=messages,
                    api_key=self.vision_api_key,
                    base_url=self.vision_base_url,
                    **kwargs,
                )
            # å¤„ç†å•å›¾åƒæ ¼å¼
            elif image_data:
                return openai_complete_if_cache(
                    self.vision_model,
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt} if system_prompt else None,
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
                                },
                            ],
                        } if image_data else {"role": "user", "content": prompt},
                    ],
                    api_key=self.vision_api_key,
                    base_url=self.vision_base_url,
                    **kwargs,
                )
            # å›é€€åˆ°çº¯æ–‡æœ¬
            else:
                return self.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

        return vision_func

    def _create_embedding_func(self):
        """åˆ›å»ºåµŒå…¥å‡½æ•°"""
        if self.use_ollama_embedding:
            # ä½¿ç”¨ Ollama åµŒå…¥
            return EmbeddingFunc(
                embedding_dim=self.embedding_dim,
                max_token_size=32000,
                func=lambda texts: ollama_embed(
                    texts,
                    embed_model=self.embedding_model,
                    api_key="",
                    host=self.embedding_host,
                ),
            )
        else:
            # ä½¿ç”¨ OpenAI å…¼å®¹çš„åµŒå…¥
            return EmbeddingFunc(
                embedding_dim=self.embedding_dim,
                max_token_size=8192,
                func=lambda texts: openai_embed(
                    texts,
                    model=self.embedding_model,
                    api_key=self.llm_api_key,
                    base_url=self.llm_base_url,
                ),
            )

    async def process_document(
        self,
        file_path: str,
        output_dir: Optional[str] = None,
        parse_method: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£

        å‚æ•°:
            file_path: æ–‡æ¡£è·¯å¾„
            output_dir: è§£æå†…å®¹çš„è¾“å‡ºç›®å½•
            parse_method: ä½¿ç”¨çš„è§£ææ–¹æ³•
            **kwargs: é¢å¤–å‚æ•°

        è¿”å›:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            await self.rag.process_document_complete(
                file_path=file_path,
                output_dir=output_dir,
                parse_method=parse_method,
                **kwargs
            )
            return {
                "success": True,
                "file_path": file_path,
                "message": f"æˆåŠŸå¤„ç†æ–‡æ¡£: {file_path}"
            }
        except Exception as e:
            raise ValueError(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")

    async def process_folder(
        self,
        folder_path: str,
        output_dir: Optional[str] = None,
        parse_method: Optional[str] = None,
        recursive: bool = True,
        max_workers: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£

        å‚æ•°:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            output_dir: è§£æå†…å®¹çš„è¾“å‡ºç›®å½•
            parse_method: ä½¿ç”¨çš„è§£ææ–¹æ³•
            recursive: é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            **kwargs: é¢å¤–å‚æ•°

        è¿”å›:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            await self.rag.process_folder_complete(
                folder_path=folder_path,
                output_dir=output_dir,
                parse_method=parse_method,
                recursive=recursive,
                max_workers=max_workers,
                **kwargs
            )
            return {
                "success": True,
                "folder_path": folder_path,
                "message": f"æˆåŠŸå¤„ç†æ–‡ä»¶å¤¹: {folder_path}"
            }
        except Exception as e:
            raise ValueError(f"å¤„ç†æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")

    async def query(
        self,
        query: str,
        mode: str = "hybrid",
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢ RAG ç³»ç»Ÿ

        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            mode: æŸ¥è¯¢æ¨¡å¼ (local, global, hybrid, naive, mix, bypass)
            **kwargs: é¢å¤–æŸ¥è¯¢å‚æ•°

        è¿”å›:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            result = await self.rag.aquery(query=query, mode=mode, **kwargs)
            return result
        except Exception as e:
            raise ValueError(f"æ‰§è¡ŒæŸ¥è¯¢å¤±è´¥: {str(e)}")

    async def query_with_multimodal(
        self,
        query: str,
        multimodal_content: List[Dict[str, Any]],
        mode: str = "hybrid",
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨å¤šæ¨¡æ€å†…å®¹æŸ¥è¯¢

        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            multimodal_content: å¤šæ¨¡æ€å†…å®¹é¡¹åˆ—è¡¨
            mode: æŸ¥è¯¢æ¨¡å¼
            **kwargs: é¢å¤–æŸ¥è¯¢å‚æ•°

        è¿”å›:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            result = await self.rag.aquery_with_multimodal(
                query=query,
                multimodal_content=multimodal_content,
                mode=mode,
                **kwargs
            )
            return result
        except Exception as e:
            raise ValueError(f"æ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢å¤±è´¥: {str(e)}")

    async def get_config_info(self) -> Dict[str, Any]:
        """
        è·å–é…ç½®ä¿¡æ¯

        è¿”å›:
            é…ç½®å­—å…¸
        """
        try:
            return self.rag.get_config_info()
        except Exception as e:
            raise ValueError(f"è·å–é…ç½®ä¿¡æ¯å¤±è´¥: {str(e)}")

    async def get_processor_info(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†å™¨ä¿¡æ¯

        è¿”å›:
            å¤„ç†å™¨ä¿¡æ¯å­—å…¸
        """
        try:
            return self.rag.get_processor_info()
        except Exception as e:
            raise ValueError(f"è·å–å¤„ç†å™¨ä¿¡æ¯å¤±è´¥: {str(e)}")

    async def finalize(self):
        """å®Œæˆå¹¶æ¸…ç†èµ„æº"""
        if not self._initialized or not self.rag:
            return
# noqa  MS80OmFIVnBZMlhsa0xUb3Y2bzZNalJxY1E9PTo0Nzk0MDQyYQ==

        try:
            logger.info("æ­£åœ¨æ¸…ç† RAGAnything èµ„æº...")
            await self.rag.finalize_storages()
            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"å®Œæˆå­˜å‚¨æ¸…ç†å¤±è´¥: {str(e)}")
            raise ValueError(f"å®Œæˆå­˜å‚¨æ¸…ç†å¤±è´¥: {str(e)}")

    async def parse_document_only(
        self,
        file_path: str,
        output_dir: Optional[str] = None,
        parse_method: Optional[str] = None,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ä»…è§£ææ–‡æ¡£è€Œä¸æ’å…¥åˆ° RAG

        å‚æ•°:
            file_path: æ–‡æ¡£è·¯å¾„
            output_dir: è§£æå†…å®¹çš„è¾“å‡ºç›®å½•
            parse_method: ä½¿ç”¨çš„è§£ææ–¹æ³•
            **kwargs: é¢å¤–å‚æ•°

        è¿”å›:
            è§£æçš„å†…å®¹åˆ—è¡¨
        """
        try:
            content_list, doc_id = await self.rag.parse_document(
                file_path=file_path,
                output_dir=output_dir,
                parse_method=parse_method,
                **kwargs
            )
            return {
                "success": True,
                "file_path": file_path,
                "doc_id": doc_id,
                "content_count": len(content_list),
                "content_list": content_list,
            }
        except Exception as e:
            raise ValueError(f"è§£ææ–‡æ¡£å¤±è´¥: {str(e)}")

    async def insert_content(
        self,
        content_list: List[Dict[str, Any]],
        file_path: str = "unknown_document",
        **kwargs
    ) -> Dict[str, Any]:
        """
        å°†é¢„è§£æçš„å†…å®¹æ’å…¥åˆ° RAG

        å‚æ•°:
            content_list: å†…å®¹é¡¹åˆ—è¡¨
            file_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå‚è€ƒï¼‰
            **kwargs: é¢å¤–å‚æ•°

        è¿”å›:
            æ’å…¥ç»“æœå­—å…¸
        """
        try:
            await self.rag.insert_content_list(
                content_list=content_list,
                file_path=file_path,
                **kwargs
            )
            return {
                "success": True,
                "file_path": file_path,
                "message": f"æˆåŠŸæ’å…¥ {len(content_list)} ä¸ªå†…å®¹é¡¹"
            }
        except Exception as e:
            raise ValueError(f"æ’å…¥å†…å®¹å¤±è´¥: {str(e)}")


class RAGAnythingContext:
    """RAG Anything MCP æœåŠ¡å™¨çš„ä¸Šä¸‹æ–‡å¯¹è±¡"""

    def __init__(self, connector: RAGAnythingConnector):
        self.connector = connector


@asynccontextmanager
async def server_lifespan(server: FastMCP) -> AsyncIterator[RAGAnythingContext]:
    """
    ç®¡ç† RAG Anything è¿æ¥å™¨çš„åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸ

    å‚æ•°:
        server: FastMCP æœåŠ¡å™¨å®ä¾‹

    ç”Ÿæˆ:
        åŒ…å«å·²åˆå§‹åŒ–è¿æ¥å™¨çš„ RAGAnythingContext
    """
    config = server.config

    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è·å–é…ç½®
    working_dir = config.get("working_dir", os.getenv("RAG_WORKING_DIR", "./rag_storage"))

    # LLM é…ç½®
    llm_api_key = config.get("llm_api_key", os.getenv("LLM_API_KEY"))
    llm_base_url = config.get("llm_base_url", os.getenv("LLM_BASE_URL"))
    llm_model = config.get("llm_model", os.getenv("LLM_MODEL", "deepseek-chat"))

    # è§†è§‰æ¨¡å‹é…ç½®
    vision_api_key = config.get("vision_api_key", os.getenv("VISION_API_KEY"))
    vision_base_url = config.get("vision_base_url", os.getenv("VISION_BASE_URL"))
    vision_model = config.get("vision_model", os.getenv("VISION_MODEL"))

    # åµŒå…¥é…ç½®
    embedding_host = config.get("embedding_host", os.getenv("EMBEDDING_HOST", "http://35.235.113.151:11434"))
    embedding_model = config.get("embedding_model", os.getenv("EMBEDDING_MODEL", "qwen3-embedding:0.6b"))
    embedding_dim = int(config.get("embedding_dim", os.getenv("EMBEDDING_DIM", "1024")))
    use_ollama_embedding = config.get("use_ollama_embedding", os.getenv("USE_OLLAMA_EMBEDDING", "true").lower() == "true")

    # è§£æå™¨é…ç½®
    parser = config.get("parser", os.getenv("RAG_PARSER", "docling"))
    parse_method = config.get("parse_method", os.getenv("RAG_PARSE_METHOD", "auto"))

    # åŠŸèƒ½æ ‡å¿—
    enable_image = config.get("enable_image_processing", os.getenv("RAG_ENABLE_IMAGE", "true").lower() == "true")
    enable_table = config.get("enable_table_processing", os.getenv("RAG_ENABLE_TABLE", "true").lower() == "true")
    enable_equation = config.get("enable_equation_processing", os.getenv("RAG_ENABLE_EQUATION", "true").lower() == "true")
# pragma: no cover  Mi80OmFIVnBZMlhsa0xUb3Y2bzZNalJxY1E9PTo0Nzk0MDQyYQ==

    # æ˜¯å¦åŠ è½½å·²æœ‰çŸ¥è¯†åº“
    load_existing = config.get("load_existing", os.getenv("LOAD_EXISTING", "true").lower() == "true")

    # åˆ›å»ºè¿æ¥å™¨
    logger.info("æ­£åœ¨åˆ›å»º RAGAnything è¿æ¥å™¨...")
    connector = RAGAnythingConnector(
        working_dir=working_dir,
        llm_api_key=llm_api_key,
        llm_base_url=llm_base_url,
        llm_model=llm_model,
        vision_api_key=vision_api_key,
        vision_base_url=vision_base_url,
        vision_model=vision_model,
        embedding_host=embedding_host,
        embedding_model=embedding_model,
        embedding_dim=embedding_dim,
        use_ollama_embedding=use_ollama_embedding,
        parser=parser,
        parse_method=parse_method,
        enable_image_processing=enable_image,
        enable_table_processing=enable_table,
        enable_equation_processing=enable_equation,
        load_existing=load_existing,
    )

    # åˆå§‹åŒ–è¿æ¥å™¨ï¼ˆåŠ è½½å·²æœ‰çŸ¥è¯†åº“æˆ–åˆ›å»ºæ–°å®ä¾‹ï¼‰
    await connector.initialize()

    try:
        yield RAGAnythingContext(connector)
    finally:
        # æ¸…ç†èµ„æº
        await connector.finalize()


# åˆ›å»º FastMCP æœåŠ¡å™¨å®ä¾‹
mcp = FastMCP(name="RAG Anything", lifespan=server_lifespan)


# ==========================================
# MCP å·¥å…·
# ==========================================

@mcp.tool()
async def process_document(
    ctx: Context,
    file_path: str,
    output_dir: str = None,
    parse_method: str = None,
) -> str:
    """
    å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶å°†å…¶æ·»åŠ åˆ° RAG ç³»ç»Ÿ

    æ­¤å·¥å…·è§£ææ–‡æ¡£ï¼ˆPDFã€å›¾åƒã€Office æ–‡ä»¶ç­‰ï¼‰å¹¶å°†å…¶å†…å®¹æ’å…¥åˆ° RAG çŸ¥è¯†åº“ä¸­ã€‚
    æ–‡æ¡£å°†è¢«è§£æä»¥æå–æ–‡æœ¬å’Œå¤šæ¨¡æ€å†…å®¹ï¼ˆå›¾åƒã€è¡¨æ ¼ã€å…¬å¼ï¼‰ã€‚

    å‚æ•°:
        file_path: è¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        output_dir: è§£æå†…å®¹çš„å¯é€‰è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        parse_method: å¯é€‰çš„è§£ææ–¹æ³• (auto, ocr, txt)ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰

    è¿”å›:
        åŒ…å«å¤„ç†è¯¦æƒ…çš„æˆåŠŸæ¶ˆæ¯
    """
    connector = ctx.request_context.lifespan_context.connector

    # éªŒè¯æ–‡ä»¶è·¯å¾„
    if not os.path.exists(file_path):
        return f"é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}"

    try:
        result = await connector.process_document(
            file_path=file_path,
            output_dir=output_dir,
            parse_method=parse_method,
        )
        return f"âœ… æˆåŠŸå¤„ç†æ–‡æ¡£: {file_path}\n\næ–‡æ¡£å·²è¢«è§£æå¹¶æ·»åŠ åˆ° RAG çŸ¥è¯†åº“ã€‚"
    except Exception as e:
        return f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def process_folder(
    ctx: Context,
    folder_path: str,
    output_dir: str = None,
    parse_method: str = None,
    recursive: bool = True,
    max_workers: int = None,
) -> str:
    """
    å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£å¹¶å°†å…¶æ·»åŠ åˆ° RAG ç³»ç»Ÿ

    æ­¤å·¥å…·å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„å¤šä¸ªæ–‡æ¡£ï¼Œè§£ææ¯ä¸ªæ–‡æ¡£å¹¶å°†å…¶å†…å®¹æ’å…¥åˆ° RAG çŸ¥è¯†åº“ä¸­ã€‚

    å‚æ•°:
        folder_path: åŒ…å«æ–‡æ¡£çš„æ–‡ä»¶å¤¹è·¯å¾„
        output_dir: è§£æå†…å®¹çš„å¯é€‰è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        parse_method: å¯é€‰çš„è§£ææ–¹æ³• (auto, ocr, txt)ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        recursive: æ˜¯å¦é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹ï¼ˆé»˜è®¤: Trueï¼‰
        max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰

    è¿”å›:
        åŒ…å«å¤„ç†è¯¦æƒ…çš„æˆåŠŸæ¶ˆæ¯
    """
    connector = ctx.request_context.lifespan_context.connector
# noqa  My80OmFIVnBZMlhsa0xUb3Y2bzZNalJxY1E9PTo0Nzk0MDQyYQ==

    # éªŒè¯æ–‡ä»¶å¤¹è·¯å¾„
    if not os.path.exists(folder_path):
        return f"é”™è¯¯: æ–‡ä»¶å¤¹æœªæ‰¾åˆ°: {folder_path}"

    if not os.path.isdir(folder_path):
        return f"é”™è¯¯: è·¯å¾„ä¸æ˜¯ç›®å½•: {folder_path}"

    try:
        result = await connector.process_folder(
            folder_path=folder_path,
            output_dir=output_dir,
            parse_method=parse_method,
            recursive=recursive,
            max_workers=max_workers,
        )
        return f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶å¤¹: {folder_path}\n\næ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£å·²è¢«è§£æå¹¶æ·»åŠ åˆ° RAG çŸ¥è¯†åº“ã€‚"
    except Exception as e:
        return f"âŒ å¤„ç†æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def query(
    ctx: Context,
    query: str,
    mode: str = "hybrid",
) -> str:
    """
    ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢ RAG ç³»ç»Ÿ

    æ­¤å·¥å…·ä½¿ç”¨æä¾›çš„æŸ¥è¯¢æ–‡æœ¬æœç´¢ RAG çŸ¥è¯†åº“å¹¶è¿”å›ç›¸å…³ä¿¡æ¯ã€‚

    å‚æ•°:
        query: è¦æœç´¢çš„æŸ¥è¯¢æ–‡æœ¬
        mode: æŸ¥è¯¢æ¨¡å¼ - ä»¥ä¸‹ä¹‹ä¸€:
            - "local": æœç´¢ç‰¹å®šå®ä½“å’Œå…³ç³»
            - "global": æœç´¢å¹¿æ³›ä¸»é¢˜å’Œæ‘˜è¦
            - "hybrid": ç»“åˆæœ¬åœ°å’Œå…¨å±€æœç´¢ï¼ˆé»˜è®¤ï¼‰
            - "naive": ç®€å•çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢
            - "mix": æ··åˆä¸åŒçš„æœç´¢ç­–ç•¥
            - "bypass": ç›´æ¥ LLM æŸ¥è¯¢ï¼Œä¸ä½¿ç”¨ RAG

    è¿”å›:
        åŒ…å«çŸ¥è¯†åº“ç›¸å…³ä¿¡æ¯çš„æŸ¥è¯¢ç»“æœ
    """
    connector = ctx.request_context.lifespan_context.connector

    # éªŒè¯æ¨¡å¼
    valid_modes = ["local", "global", "hybrid", "naive", "mix", "bypass"]
    if mode not in valid_modes:
        return f"é”™è¯¯: æ— æ•ˆçš„æ¨¡å¼ '{mode}'ã€‚å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {', '.join(valid_modes)}"

    try:
        result = await connector.query(query=query, mode=mode)
        return result
    except Exception as e:
        return f"âŒ æ‰§è¡ŒæŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def query_with_images(
    ctx: Context,
    query: str,
    image_paths: List[str],
    mode: str = "hybrid",
) -> str:
    """
    ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæŸ¥è¯¢ RAG ç³»ç»Ÿ

    æ­¤å·¥å…·æ‰§è¡Œç»“åˆæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€æŸ¥è¯¢ï¼Œé€‚ç”¨äºå…³äºæ–‡æ¡£ä¸­è§†è§‰å†…å®¹çš„é—®é¢˜ã€‚

    å‚æ•°:
        query: æŸ¥è¯¢æ–‡æœ¬
        image_paths: è¦åŒ…å«åœ¨æŸ¥è¯¢ä¸­çš„å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        mode: æŸ¥è¯¢æ¨¡å¼ (local, global, hybrid, naive, mix, bypass)

    è¿”å›:
        ç»“åˆæ–‡æœ¬å’Œå›¾åƒåˆ†æçš„æŸ¥è¯¢ç»“æœ
    """
    connector = ctx.request_context.lifespan_context.connector

    # éªŒè¯æ¨¡å¼
    valid_modes = ["local", "global", "hybrid", "naive", "mix", "bypass"]
    if mode not in valid_modes:
        return f"é”™è¯¯: æ— æ•ˆçš„æ¨¡å¼ '{mode}'ã€‚å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {', '.join(valid_modes)}"

    # éªŒè¯å›¾åƒè·¯å¾„
    for img_path in image_paths:
        if not os.path.exists(img_path):
            return f"é”™è¯¯: å›¾åƒæ–‡ä»¶æœªæ‰¾åˆ°: {img_path}"

    # æ„å»ºå¤šæ¨¡æ€å†…å®¹
    multimodal_content = [
        {"type": "image", "img_path": img_path}
        for img_path in image_paths
    ]

    try:
        result = await connector.query_with_multimodal(
            query=query,
            multimodal_content=multimodal_content,
            mode=mode,
        )
        return result
    except Exception as e:
        return f"âŒ æ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def get_config(ctx: Context) -> str:
    """
    è·å–å½“å‰ RAG Anything é…ç½®

    è¿”å›å½“å‰é…ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
    - å·¥ä½œç›®å½•
    - è§£æå™¨è®¾ç½®
    - å¤šæ¨¡æ€å¤„ç†è®¾ç½®
    - æ‰¹å¤„ç†è®¾ç½®
    - ä¸Šä¸‹æ–‡æå–è®¾ç½®

    è¿”å›:
        åŒ…å«é…ç½®è¯¦æƒ…çš„ JSON å­—ç¬¦ä¸²
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        config_info = await connector.get_config_info()
        return json.dumps(config_info, indent=2, ensure_ascii=False)
    except Exception as e:
        return f"âŒ è·å–é…ç½®æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def get_processor_status(ctx: Context) -> str:
    """
    è·å–å¤„ç†å™¨çŠ¶æ€ä¿¡æ¯

    è¿”å›å¤„ç†å™¨å½“å‰çŠ¶æ€çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
    - å¯ç”¨çš„å¤„ç†å™¨
    - å¤„ç†å™¨é…ç½®
    - å¤„ç†èƒ½åŠ›

    è¿”å›:
        åŒ…å«å¤„ç†å™¨çŠ¶æ€è¯¦æƒ…çš„ JSON å­—ç¬¦ä¸²
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        processor_info = await connector.get_processor_info()
        return json.dumps(processor_info, indent=2, ensure_ascii=False)
    except Exception as e:
        return f"âŒ è·å–å¤„ç†å™¨çŠ¶æ€æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def list_supported_formats(ctx: Context) -> str:
    """
    åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

    è¿”å› RAG ç³»ç»Ÿå¯ä»¥å¤„ç†çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ã€‚

    è¿”å›:
        æ ¼å¼åŒ–çš„æ”¯æŒæ–‡ä»¶æ ¼å¼åˆ—è¡¨
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        extensions = connector.rag.get_supported_file_extensions()

        result = "ğŸ“„ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:\n\n"
        result += "æ–‡æ¡£:\n"
        result += "  - PDF: .pdf\n"
        result += "  - Office: .doc, .docx, .ppt, .pptx, .xls, .xlsx\n"
        result += "  - æ–‡æœ¬: .txt, .md\n"
        result += "  - HTML: .html, .htm\n\n"
        result += "å›¾åƒ:\n"
        result += "  - .png, .jpg, .jpeg, .bmp, .tiff, .gif, .webp\n\n"
        result += f"æ”¯æŒçš„æ‰©å±•åæ€»æ•°: {len(extensions)}\n"
        result += f"æ‰©å±•å: {', '.join(sorted(extensions))}"

        return result
    except Exception as e:
        return f"âŒ åˆ—å‡ºæ”¯æŒæ ¼å¼æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def parse_document_only(
    ctx: Context,
    file_path: str,
    output_dir: str = None,
    parse_method: str = None,
) -> str:
    """
    ä»…è§£ææ–‡æ¡£è€Œä¸å°†å…¶æ’å…¥åˆ° RAG ç³»ç»Ÿ

    æ­¤å·¥å…·ä»…è§£ææ–‡æ¡£å¹¶è¿”å›è§£æçš„å†…å®¹ç»“æ„ï¼Œè€Œä¸å°†å…¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    é€‚ç”¨äºé¢„è§ˆæ–‡æ¡£ç»“æ„æˆ–åœ¨æ’å…¥å‰å¤„ç†å†…å®¹ã€‚

    å‚æ•°:
        file_path: è¦è§£æçš„æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        output_dir: è§£æå†…å®¹çš„å¯é€‰è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
        parse_method: å¯é€‰çš„è§£ææ–¹æ³• (auto, ocr, txt)ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰

    è¿”å›:
        åŒ…å«è§£æå†…å®¹ä¿¡æ¯çš„ JSON å­—ç¬¦ä¸²
    """
    connector = ctx.request_context.lifespan_context.connector

    # éªŒè¯æ–‡ä»¶è·¯å¾„
    if not os.path.exists(file_path):
        return f"é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}"

    try:
        result = await connector.parse_document_only(
            file_path=file_path,
            output_dir=output_dir,
            parse_method=parse_method,
        )

        # æ ¼å¼åŒ–å“åº”
        response = f"âœ… æˆåŠŸè§£ææ–‡æ¡£: {file_path}\n\n"
        response += f"æ–‡æ¡£ ID: {result['doc_id']}\n"
        response += f"å†…å®¹é¡¹æ•°é‡: {result['content_count']}\n\n"
        response += "å†…å®¹ç»“æ„:\n"

        # ç»Ÿè®¡å†…å®¹ç±»å‹
        content_types = {}
        for item in result['content_list']:
            content_type = item.get('type', 'unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1

        for content_type, count in sorted(content_types.items()):
            response += f"  - {content_type}: {count}\n"

        return response
    except Exception as e:
        return f"âŒ è§£ææ–‡æ¡£æ—¶å‡ºé”™: {str(e)}"


@mcp.tool()
async def insert_parsed_content(
    ctx: Context,
    content_json: str,
    file_path: str = "unknown_document",
) -> str:
    """
    å°†é¢„è§£æçš„å†…å®¹æ’å…¥åˆ° RAG ç³»ç»Ÿ

    æ­¤å·¥å…·å…è®¸æ‚¨æ’å…¥å…ˆå‰è§£ææˆ–æ‰‹åŠ¨æ„å»ºçš„å†…å®¹ï¼Œè€Œæ— éœ€é‡æ–°è§£æåŸå§‹æ–‡æ¡£ã€‚

    å‚æ•°:
        content_json: åŒ…å«å†…å®¹åˆ—è¡¨çš„ JSON å­—ç¬¦ä¸²
        file_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå‚è€ƒï¼‰ï¼ˆé»˜è®¤: "unknown_document"ï¼‰

    è¿”å›:
        åŒ…å«æ’å…¥è¯¦æƒ…çš„æˆåŠŸæ¶ˆæ¯
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        # è§£æ JSON å†…å®¹
        content_list = json.loads(content_json)

        if not isinstance(content_list, list):
            return "é”™è¯¯: content_json å¿…é¡»æ˜¯ JSON æ•°ç»„"

        result = await connector.insert_content(
            content_list=content_list,
            file_path=file_path,
        )

        return f"âœ… æˆåŠŸä» {file_path} æ’å…¥ {len(content_list)} ä¸ªå†…å®¹é¡¹"
    except json.JSONDecodeError as e:
        return f"âŒ è§£æ JSON æ—¶å‡ºé”™: {str(e)}"
    except Exception as e:
        return f"âŒ æ’å…¥å†…å®¹æ—¶å‡ºé”™: {str(e)}"


# ==========================================
# ä¸»å…¥å£ç‚¹
# ==========================================

def main():
    """MCP æœåŠ¡å™¨çš„ä¸»å…¥å£ç‚¹"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="RAG Anything MCP æœåŠ¡å™¨ - å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†å’ŒæŸ¥è¯¢"
    )
    parser.add_argument(
        "--working-dir",
        type=str,
        default=os.getenv("RAG_WORKING_DIR", "./rag_storage"),
        help="RAG å­˜å‚¨çš„å·¥ä½œç›®å½• (é»˜è®¤: ./rag_storage)",
    )
    parser.add_argument(
        "--llm-api-key",
        type=str,
        default=os.getenv("LLM_API_KEY"),
        help="LLM çš„ API å¯†é’¥",
    )
    parser.add_argument(
        "--llm-base-url",
        type=str,
        default=os.getenv("LLM_BASE_URL"),
        help="LLM API çš„åŸºç¡€ URL",
    )
    parser.add_argument(
        "--llm-model",
        type=str,
        default=os.getenv("LLM_MODEL", "deepseek-chat"),
        help="LLM æ¨¡å‹åç§° (é»˜è®¤: deepseek-chat)",
    )
    parser.add_argument(
        "--vision-model",
        type=str,
        default=os.getenv("VISION_MODEL"),
        help="è§†è§‰æ¨¡å‹åç§° (å¯é€‰)",
    )
    parser.add_argument(
        "--embedding-host",
        type=str,
        default=os.getenv("EMBEDDING_HOST", "http://35.235.113.151:11434"),
        help="Ollama åµŒå…¥æœåŠ¡å™¨åœ°å€ (é»˜è®¤: http://35.235.113.151:11434)",
    )
    parser.add_argument(
        "--embedding-model",
        type=str,
        default=os.getenv("EMBEDDING_MODEL", "qwen3-embedding:0.6b"),
        help="åµŒå…¥æ¨¡å‹åç§° (é»˜è®¤: qwen3-embedding:0.6b)",
    )
    parser.add_argument(
        "--embedding-dim",
        type=int,
        default=int(os.getenv("EMBEDDING_DIM", "1024")),
        help="åµŒå…¥ç»´åº¦ (é»˜è®¤: 1024)",
    )
    parser.add_argument(
        "--use-ollama-embedding",
        action="store_true",
        default=os.getenv("USE_OLLAMA_EMBEDDING", "true").lower() == "true",
        help="ä½¿ç”¨ Ollama åµŒå…¥ (é»˜è®¤: True)",
    )
    parser.add_argument(
        "--parser",
        type=str,
        choices=["mineru", "docling"],
        default=os.getenv("RAG_PARSER", "docling"),
        help="ä½¿ç”¨çš„è§£æå™¨ (é»˜è®¤: docling)",
    )
    parser.add_argument(
        "--parse-method",
        type=str,
        choices=["auto", "ocr", "txt"],
        default=os.getenv("RAG_PARSE_METHOD", "auto"),
        help="è§£ææ–¹æ³• (é»˜è®¤: auto)",
    )
    parser.add_argument(
        "--load-existing",
        action="store_true",
        default=os.getenv("LOAD_EXISTING", "true").lower() == "true",
        help="åŠ è½½å·²å­˜åœ¨çš„çŸ¥è¯†åº“ (é»˜è®¤: True)",
    )

    args = parser.parse_args()

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ›´æ–°æœåŠ¡å™¨é…ç½®
    mcp.config = {
        "working_dir": args.working_dir,
        "llm_api_key": args.llm_api_key,
        "llm_base_url": args.llm_base_url,
        "llm_model": args.llm_model,
        "vision_model": args.vision_model,
        "embedding_host": args.embedding_host,
        "embedding_model": args.embedding_model,
        "embedding_dim": args.embedding_dim,
        "use_ollama_embedding": args.use_ollama_embedding,
        "parser": args.parser,
        "parse_method": args.parse_method,
        "load_existing": args.load_existing,
    }

    # è¿è¡ŒæœåŠ¡å™¨
    mcp.run(transport="sse")


if __name__ == "__main__":
    main()
