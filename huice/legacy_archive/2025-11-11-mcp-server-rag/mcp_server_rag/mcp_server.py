"""
ä¸“ä¸šçš„ RAG MCP æœåŠ¡å™¨ - å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–

æœ¬ MCP æœåŠ¡å™¨æä¾›é«˜çº§ RAG åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å¤šç­–ç•¥æŸ¥è¯¢æ”¹å†™
- ä»å‘é‡æ•°æ®åº“å¹¶è¡Œæ£€ç´¢
- ä½¿ç”¨ LLM è¯„ä¼°çš„æ™ºèƒ½é‡æ’åº
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç­”æ¡ˆåˆæˆ

æ¶æ„ï¼š
1. æŸ¥è¯¢æ”¹å†™å™¨ï¼šç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
2. å¹¶è¡Œæ£€ç´¢ï¼šä»çŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢
3. é‡æ’åºå™¨ï¼šå»é‡å’Œé‡æ’åºç»“æœ
4. åˆæˆå™¨ï¼šç”Ÿæˆå¸¦å¼•ç”¨çš„æœ€ç»ˆç­”æ¡ˆ
"""
"""
Copyright (c) 2025 Dean Wu. All rights reserved.
AuroraAI Project.
"""


import argparse
import os
from contextlib import asynccontextmanager
from typing import Any, AsyncIterator, Optional, List, Dict
from dotenv import load_dotenv
import json
import asyncio

from fastmcp import FastMCP, Context
from pydantic import BaseModel, Field

from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_milvus import Milvus, BM25BuiltInFunction
from langchain_ollama import OllamaEmbeddings


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

class RewrittenQuery(BaseModel):
    """æ”¹å†™åçš„æŸ¥è¯¢åŠå…¶ç­–ç•¥"""
    original: str = Field(description="åŸå§‹æŸ¥è¯¢")
    rewritten: str = Field(description="æ”¹å†™åçš„æŸ¥è¯¢")
    strategy: str = Field(description="æ”¹å†™ç­–ç•¥ï¼šsimplify, expand, rephrase, decompose")


class QueryRewriteOutput(BaseModel):
    """æŸ¥è¯¢æ”¹å†™çš„è¾“å‡º"""
    queries: List[RewrittenQuery] = Field(description="3-5ä¸ªæŸ¥è¯¢å˜ä½“çš„åˆ—è¡¨")
# fmt: off  MC80OmFIVnBZMlhsa0xUb3Y2bzZlalY0VlE9PToxYjYxODkyMQ==


class RetrievalResult(BaseModel):
    """å•ä¸ªæ£€ç´¢ç»“æœ"""
    query: str = Field(description="ç”¨äºæ£€ç´¢çš„æŸ¥è¯¢")
    content: str = Field(description="æ£€ç´¢åˆ°çš„å†…å®¹")
    score: float = Field(description="ç›¸å…³æ€§åˆ†æ•°")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="æ–‡æ¡£å…ƒæ•°æ®")
    strategy: Optional[str] = Field(default=None, description="ä½¿ç”¨çš„æŸ¥è¯¢ç­–ç•¥")


class RAGResponse(BaseModel):
    """å®Œæ•´çš„ RAG å“åº”"""
    answer: str = Field(description="æœ€ç»ˆåˆæˆçš„ç­”æ¡ˆ")
    sources: List[Dict[str, Any]] = Field(description="ä½¿ç”¨çš„æºæ–‡æ¡£")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="å“åº”å…ƒæ•°æ®")


# ============================================================================
# RAG è¿æ¥å™¨
# ============================================================================

class RAGConnector:
    """ç®¡ç† RAG æ“ä½œï¼ŒåŒ…æ‹¬ LLM å’Œå‘é‡å­˜å‚¨è¿æ¥"""

    def __init__(
        self,
        llm_provider: str = "deepseek",
        llm_model: str = "deepseek-chat",
        llm_api_key: Optional[str] = None,
        embedding_model: str = "qwen3-embedding:0.6b",
        embedding_base_url: str = "http://35.235.113.151:11434",
        milvus_uri: str = "http://35.235.113.151:19530",
        milvus_collection: Optional[str] = None,
    ):
        """ä½¿ç”¨ LLM å’Œå‘é‡å­˜å‚¨åˆå§‹åŒ– RAG è¿æ¥å™¨"""

        # åˆå§‹åŒ– LLM
        if llm_api_key:
            os.environ[f"{llm_provider.upper()}_API_KEY"] = llm_api_key

        self.llm = init_chat_model(f"{llm_provider}:{llm_model}")

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OllamaEmbeddings(
            model=embedding_model,
            base_url=embedding_base_url
        )
# pylint: disable  MS80OmFIVnBZMlhsa0xUb3Y2bzZlalY0VlE9PToxYjYxODkyMQ==

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.milvus_uri = milvus_uri
        self.milvus_collection = milvus_collection
        self.vector_store = None

        if milvus_collection:
            self._init_vector_store(milvus_collection)

    def _init_vector_store(self, collection_name: str):
        """ä½¿ç”¨é›†åˆåˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        self.vector_store = Milvus(
            embedding_function=self.embeddings,
            connection_args={"uri": self.milvus_uri},
            collection_name=collection_name,
            builtin_function=BM25BuiltInFunction(),
            vector_field=["dense", "sparse"],
        )

    def set_collection(self, collection_name: str):
        """åˆ‡æ¢åˆ°ä¸åŒçš„é›†åˆ"""
        print(f"[DEBUG] set_collection è¢«è°ƒç”¨: collection_name={collection_name}")
        print(f"[DEBUG] å½“å‰ vector_store çŠ¶æ€: {self.vector_store is not None}")
        self.milvus_collection = collection_name
        self._init_vector_store(collection_name)
        print(f"[DEBUG] vector_store åˆå§‹åŒ–å®Œæˆ: {self.vector_store is not None}")
    
    async def rewrite_query(self, question: str, num_variants: int = 3) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨ä¸åŒç­–ç•¥å°†æŸ¥è¯¢æ”¹å†™ä¸ºå¤šä¸ªå˜ä½“

        å‚æ•°ï¼š
            question: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
            num_variants: è¦ç”Ÿæˆçš„æŸ¥è¯¢å˜ä½“æ•°é‡ï¼ˆ3-5ä¸ªï¼‰

        è¿”å›ï¼š
            å¸¦æœ‰ç­–ç•¥çš„æ”¹å†™æŸ¥è¯¢åˆ—è¡¨
        """
        structured_llm = self.llm.with_structured_output(QueryRewriteOutput)

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚å°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™ä¸º {num_variants} ä¸ªä¸åŒçš„æŸ¥è¯¢å˜ä½“ï¼Œä»¥æé«˜æ£€ç´¢å‡†ç¡®ç‡ã€‚

ç­–ç•¥ï¼š
1. original - ä¿ç•™åŸå§‹æŸ¥è¯¢
2. simplify - ç®€åŒ–æŸ¥è¯¢ï¼Œæå–æ ¸å¿ƒå…³é”®è¯
3. expand - æ‰©å±•æŸ¥è¯¢ï¼Œæ·»åŠ ç›¸å…³æ¦‚å¿µå’ŒåŒä¹‰è¯
4. rephrase - ä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼é‡æ–°è¡¨è¿°
5. decompose - å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜

è¦æ±‚ï¼š
- æ¯ä¸ªå˜ä½“åº”ä½¿ç”¨ä¸åŒçš„ç­–ç•¥
- æ”¹å†™åçš„æŸ¥è¯¢åº”æ›´å¥½åœ°åŒ¹é…å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£
- ä¿æŒè¯­ä¹‰å«ä¹‰
- ç”Ÿæˆ {num_variants} ä¸ªæŸ¥è¯¢å˜ä½“"""

        user_prompt = f"å°†ä»¥ä¸‹é—®é¢˜æ”¹å†™ä¸ºå¤šä¸ªæŸ¥è¯¢å˜ä½“ï¼š\n\n{question}"

        try:
            response = structured_llm.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ])

            return [q.model_dump() for q in response.queries]

        except Exception as e:
            # é™çº§å¤„ç†ï¼šä»…ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            return [{
                "original": question,
                "rewritten": question,
                "strategy": "original"
            }]
    
    def _ensure_vector_store(self):
        """ç¡®ä¿ vector_store å·²åˆå§‹åŒ–ï¼Œå¦‚æœæœªåˆå§‹åŒ–åˆ™æŠ›å‡ºå‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
        if not self.vector_store:
            error_msg = (
                "âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–\n\n"
                "è¯·å…ˆè°ƒç”¨ rag_set_collection å·¥å…·è®¾ç½®é›†åˆï¼Œä¾‹å¦‚ï¼š\n"
                "  rag_set_collection(collection_name='your_collection_name')\n\n"
                f"å½“å‰çŠ¶æ€ï¼š\n"
                f"  - Milvus URI: {self.milvus_uri}\n"
                f"  - é›†åˆåç§°: {self.milvus_collection or 'æœªè®¾ç½®'}\n"
                f"  - Vector Store: {'å·²åˆå§‹åŒ–' if self.vector_store else 'æœªåˆå§‹åŒ–'}"
            )
            raise ValueError(error_msg)

    async def retrieve_documents(
        self,
        query: str,
        k: int = 3,
        strategy: Optional[str] = None
    ) -> List[RetrievalResult]:
        """
        ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢æ–‡æ¡£

        å‚æ•°ï¼š
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¦æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            strategy: ä½¿ç”¨çš„æŸ¥è¯¢ç­–ç•¥

        è¿”å›ï¼š
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        print(f"[DEBUG] retrieve_documents è¢«è°ƒç”¨")
        print(f"[DEBUG] vector_store çŠ¶æ€: {self.vector_store is not None}")
        print(f"[DEBUG] milvus_collection: {self.milvus_collection}")

        # ç¡®ä¿ vector_store å·²åˆå§‹åŒ–
        self._ensure_vector_store()

        try:
            docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)

            results = []
            for doc, score in docs_with_scores:
                result = RetrievalResult(
                    query=query,
                    content=doc.page_content,
                    score=float(score),
                    metadata=doc.metadata,
                    strategy=strategy
                )
                results.append(result)

            return results
# noqa  Mi80OmFIVnBZMlhsa0xUb3Y2bzZlalY0VlE9PToxYjYxODkyMQ==

        except Exception as e:
            raise ValueError(f"æ£€ç´¢å¤±è´¥ï¼š{str(e)}")
    
    async def rerank_results(
        self,
        results: List[RetrievalResult],
        question: str,
        top_k: int = 10
    ) -> List[RetrievalResult]:
        """
        é‡æ’åºå’Œå»é‡ç»“æœ

        å‚æ•°ï¼š
            results: æ£€ç´¢ç»“æœåˆ—è¡¨
            question: åŸå§‹é—®é¢˜
            top_k: è¦è¿”å›çš„é¡¶éƒ¨ç»“æœæ•°é‡

        è¿”å›ï¼š
            é‡æ’åºå’Œå»é‡åçš„ç»“æœ
        """
        if not results:
            return []

        # é€šè¿‡ pkï¼ˆä¸»é”®ï¼‰å»é‡
        seen_pks = set()
        deduped = []

        for result in results:
            pk = result.metadata.get("pk")
            if pk is not None:
                if pk not in seen_pks:
                    seen_pks.add(pk)
                    deduped.append(result)
            else:
                deduped.append(result)

        # æŒ‰åˆ†æ•°æ’åºï¼ˆé™åºï¼‰
        deduped.sort(key=lambda x: x.score, reverse=True)

        # å¤šæ ·æ€§ä¼˜åŒ–ï¼šä¼˜å…ˆé€‰æ‹©ä¸åŒæ¥æº
        diverse_results = []
        seen_sources = set()
        remaining = []

        for result in deduped:
            source = result.metadata.get("source", "unknown")
            if source not in seen_sources:
                diverse_results.append(result)
                seen_sources.add(source)
            else:
                remaining.append(result)

        diverse_results.extend(remaining)

        return diverse_results[:top_k]
    
    async def synthesize_answer(
        self,
        question: str,
        results: List[RetrievalResult],
        top_n: int = 5
    ) -> RAGResponse:
        """
        ä»æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆæˆæœ€ç»ˆç­”æ¡ˆ

        å‚æ•°ï¼š
            question: åŸå§‹é—®é¢˜
            results: é‡æ’åºåçš„æ£€ç´¢ç»“æœ
            top_n: ç”¨äºåˆæˆçš„é¡¶éƒ¨æ–‡æ¡£æ•°é‡

        è¿”å›ï¼š
            åŒ…å«ç­”æ¡ˆå’Œæ¥æºçš„ RAG å“åº”
        """
        if not results:
            return RAGResponse(
                answer="æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·å°è¯•æ¢ä¸€ç§æ–¹å¼æé—®ã€‚",
                sources=[],
                metadata={"num_results": 0}
            )

        # ä» top-n ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        sources = []

        for i, result in enumerate(results[:top_n], 1):
            source = result.metadata.get('source', 'unknown')
            content = result.content
            score = result.score

            context_part = f"""[æ–‡æ¡£ {i}]
æ¥æºï¼š{source}
ç›¸å…³æ€§åˆ†æ•°ï¼š{score:.3f}
å†…å®¹ï¼š
{content}"""
            context_parts.append(context_part)

            sources.append({
                "document_id": i,
                "source": source,
                "score": score,
                "content_preview": content[:200]
            })

        context = "\n\n" + "="*80 + "\n\n".join(context_parts)

        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. **åŸºäºäº‹å®**ï¼šç­”æ¡ˆå¿…é¡»ä¸¥æ ¼åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä¸è¦ç¼–é€ 
2. **å¼•ç”¨æ¥æº**ï¼šä½¿ç”¨ [æ–‡æ¡£ X] æ ¼å¼æ ‡æ³¨ä¿¡æ¯æ¥æº
3. **ç»¼åˆä¿¡æ¯**ï¼šå¦‚æœå¤šä¸ªæ–‡æ¡£æä¾›äº†ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»¼åˆæ‰€æœ‰ä¿¡æ¯
4. **æ¸…æ™°å‡†ç¡®**ï¼šä½¿ç”¨è¦ç‚¹æˆ–æ®µè½æ¸…æ™°åœ°ç»„ç»‡ç­”æ¡ˆ
5. **è¯šå®è¡¨è¾¾**ï¼šå¦‚æœæ–‡æ¡£ç¼ºä¹è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜

ç­”æ¡ˆç»“æ„ï¼š
- å¼€å¤´ï¼šç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜
- ä¸­é—´ï¼šæä¾›è¯¦ç»†è§£é‡Šå’Œæ”¯æŒä¿¡æ¯
- ç»“å°¾ï¼šæ€»ç»“è¦ç‚¹æˆ–æ·»åŠ è¡¥å……è¯´æ˜"""
# type: ignore  My80OmFIVnBZMlhsa0xUb3Y2bzZlalY0VlE9PToxYjYxODkyMQ==

        user_prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†å‡†ç¡®çš„ç­”æ¡ˆï¼š"""

        try:
            response = self.llm.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ])

            answer = response.content.strip()

            # è®¡ç®—å…ƒæ•°æ®
            unique_sources = set(r.metadata.get('source', 'unknown') for r in results)
            avg_score = sum(r.score for r in results[:top_n]) / min(top_n, len(results))

            return RAGResponse(
                answer=answer,
                sources=sources,
                metadata={
                    "num_results": len(results),
                    "num_sources": len(unique_sources),
                    "avg_score": avg_score,
                    "top_n_used": min(top_n, len(results))
                }
            )

        except Exception as e:
            # é™çº§å¤„ç†ï¼šè¿”å›ç»“æ„åŒ–æ‘˜è¦
            summary = f"# æ¥è‡ª {len(results)} ä¸ªç›¸å…³æ–‡æ¡£çš„ä¿¡æ¯æ‘˜è¦\n\n"
            summary += f"**é—®é¢˜**ï¼š{question}\n\n"
            summary += f"**æ³¨æ„**ï¼šLLM ç”Ÿæˆå¤±è´¥ï¼Œä»¥ä¸‹æ˜¯æ–‡æ¡£æ‘˜è¦ï¼š\n\n"

            for i, result in enumerate(results[:5], 1):
                source = result.metadata.get('source', 'unknown')
                content_preview = result.content[:300].replace('\n', ' ')
                if len(result.content) > 300:
                    content_preview += "..."

                summary += f"## æ–‡æ¡£ {i}ï¼ˆç›¸å…³æ€§ï¼š{result.score:.3f}ï¼‰\n"
                summary += f"**æ¥æº**ï¼š{source}\n"
                summary += f"**å†…å®¹**ï¼š{content_preview}\n\n"

            return RAGResponse(
                answer=summary,
                sources=sources,
                metadata={
                    "error": str(e),
                    "num_results": len(results)
                }
            )


# ============================================================================
# MCP æœåŠ¡å™¨è®¾ç½®
# ============================================================================

class RAGContext:
    """RAG æ“ä½œçš„ä¸Šä¸‹æ–‡"""
    def __init__(self, connector: RAGConnector):
        self.connector = connector


@asynccontextmanager
async def server_lifespan(server: FastMCP) -> AsyncIterator[RAGContext]:
    """ç®¡ç† RAG è¿æ¥å™¨çš„åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸ"""
    config = server.config

    connector = RAGConnector(
        llm_provider=config.get("llm_provider", "deepseek"),
        llm_model=config.get("llm_model", "deepseek-chat"),
        llm_api_key=config.get("llm_api_key"),
        embedding_model=config.get("embedding_model", "qwen3-embedding:0.6b"),
        embedding_base_url=config.get("embedding_base_url", "http://35.235.113.151:11434"),
        milvus_uri=config.get("milvus_uri", "http://35.235.113.151:19530"),
        milvus_collection=config.get("milvus_collection"),
    )

    try:
        yield RAGContext(connector)
    finally:
        pass


mcp = FastMCP(name="RAG", lifespan=server_lifespan)


# ============================================================================
# MCP å·¥å…·
# ============================================================================

# # @mcp.tool()
# async def rag_get_status(
#     ctx: Context = None
# ) -> str:
#     """
#     è·å– RAG ç³»ç»Ÿçš„å½“å‰çŠ¶æ€ã€‚
#
#     è¿”å›å½“å‰é›†åˆåç§°å’Œå‘é‡å­˜å‚¨çš„åˆå§‹åŒ–çŠ¶æ€ã€‚
#     """
#     connector = ctx.request_context.lifespan_context.connector
#
#     status = f"""
# ğŸ“Š RAG ç³»ç»ŸçŠ¶æ€
# {'='*60}
# ğŸ”— Connector å¯¹è±¡ ID: {id(connector)}
# ğŸ“ å½“å‰é›†åˆ: {connector.milvus_collection or 'æœªè®¾ç½®'}
# âœ… Vector Store çŠ¶æ€: {'å·²åˆå§‹åŒ–' if connector.vector_store else 'æœªåˆå§‹åŒ–'}
# ğŸŒ Milvus URI: {connector.milvus_uri}
# {'='*60}
# """
#     return status


# # @mcp.tool()
# async def rag_set_collection(
#     collection_name: str,
#     ctx: Context = None
# ) -> str:
#     """
#     è®¾ç½®ç”¨äº RAG æ“ä½œçš„ Milvus é›†åˆã€‚
#
#     å‚æ•°ï¼š
#         collection_name: Milvus é›†åˆçš„åç§°
#     """
#     connector = ctx.request_context.lifespan_context.connector
#     print(f"[DEBUG] rag_set_collection å·¥å…·è¢«è°ƒç”¨")
#     print(f"[DEBUG] connector å¯¹è±¡ ID: {id(connector)}")
#
#     try:
#         connector.set_collection(collection_name)
#         # éªŒè¯è®¾ç½®æ˜¯å¦æˆåŠŸ
#         if connector.vector_store is None:
#             return f"âš ï¸ è­¦å‘Šï¼šé›†åˆè®¾ç½®å¯èƒ½å¤±è´¥ï¼Œvector_store ä»ä¸º None"
#         return f"âœ… æˆåŠŸè®¾ç½®é›†åˆä¸º '{collection_name}'"
#     except Exception as e:
#         import traceback
#         error_detail = traceback.format_exc()
#         print(f"[ERROR] è®¾ç½®é›†åˆå¤±è´¥: {error_detail}")
#         return f"âŒ è®¾ç½®é›†åˆå¤±è´¥ï¼š{str(e)}"
#

@mcp.tool()
async def rag_query_rewrite(
    question: str,
    num_variants: int = 3,
    ctx: Context = None
) -> str:
    """
    ä½¿ç”¨ä¸åŒç­–ç•¥å°†ç”¨æˆ·é—®é¢˜æ”¹å†™ä¸ºå¤šä¸ªæŸ¥è¯¢å˜ä½“ã€‚

    é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„æŸ¥è¯¢è¡¨è¿°æ¥æé«˜æ£€ç´¢å‡†ç¡®ç‡ã€‚

    å‚æ•°ï¼š
        question: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
        num_variants: è¦ç”Ÿæˆçš„æŸ¥è¯¢å˜ä½“æ•°é‡ï¼ˆ3-5ä¸ªï¼‰
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        queries = await connector.rewrite_query(question, num_variants)

        output = f"æŸ¥è¯¢æ”¹å†™ç»“æœï¼ˆ{len(queries)} ä¸ªå˜ä½“ï¼‰ï¼š\n\n"
        output += f"åŸå§‹é—®é¢˜ï¼š{question}\n\n"
        output += "æ”¹å†™åçš„æŸ¥è¯¢ï¼š\n"

        for i, q in enumerate(queries, 1):
            output += f"\n{i}. [{q['strategy']}]\n"
            output += f"   {q['rewritten']}\n"

        return output

    except Exception as e:
        return f"æŸ¥è¯¢æ”¹å†™å¤±è´¥ï¼š{str(e)}"


@mcp.tool()
async def rag_retrieve(
    query: str,
    collection_name: Optional[str],
    k: int = 5,
    ctx: Context = None
) -> str:
    """
    ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚

    å‚æ•°ï¼š
        query: è¦æœç´¢çš„æŸ¥è¯¢æ–‡æœ¬
        collection_name: å¯é€‰çš„é›†åˆåç§°ï¼ˆå¦‚æœæœªå…¨å±€è®¾ç½®ï¼‰
        k: è¦æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
    """
    connector = ctx.request_context.lifespan_context.connector
    print(f"[DEBUG] rag_retrieve å·¥å…·è¢«è°ƒç”¨")
    print(f"[DEBUG] connector å¯¹è±¡ ID: {id(connector)}")
    print(f"[DEBUG] collection_name å‚æ•°: {collection_name}")
    print(f"[DEBUG] å½“å‰ vector_store çŠ¶æ€: {connector.vector_store is not None}")

    try:
        # å¦‚æœæä¾›äº†é›†åˆåç§°ï¼Œåˆ™è®¾ç½®é›†åˆ
        if collection_name:
            print(f"[DEBUG] è®¾ç½®é›†åˆ: {collection_name}")
            connector.set_collection(collection_name)
        elif not connector.vector_store:
            # å¦‚æœæ²¡æœ‰æä¾› collection_name ä¸” vector_store æœªåˆå§‹åŒ–ï¼Œè¿”å›å‹å¥½æç¤º
            return (
                "âŒ æ£€ç´¢å¤±è´¥ï¼šå‘é‡å­˜å‚¨æœªåˆå§‹åŒ–\n\n"
                "è¯·å…ˆæ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š\n"
                "1. è°ƒç”¨ rag_set_collection è®¾ç½®é›†åˆï¼š\n"
                "   rag_set_collection(collection_name='your_collection_name')\n\n"
                "2. æˆ–è€…åœ¨è°ƒç”¨ rag_retrieve æ—¶æä¾› collection_name å‚æ•°ï¼š\n"
                "   rag_retrieve(query='...', collection_name='your_collection_name')\n\n"
                f"å½“å‰çŠ¶æ€ï¼š\n"
                f"  - Milvus URI: {connector.milvus_uri}\n"
                f"  - é›†åˆåç§°: {connector.milvus_collection or 'æœªè®¾ç½®'}"
            )

        results = await connector.retrieve_documents(query, k=k)

        output = f"âœ… æ£€ç´¢ç»“æœï¼ˆ{len(results)} ä¸ªæ–‡æ¡£ï¼‰ï¼š\n\n"
        output += f"ğŸ“ æŸ¥è¯¢ï¼š{query}\n"
        output += f"ğŸ“ é›†åˆï¼š{connector.milvus_collection}\n\n"

        for i, result in enumerate(results, 1):
            source = result.metadata.get('source', 'unknown')
            output += f"\n{i}. [åˆ†æ•°ï¼š{result.score:.3f}] {source}\n"
            content_preview = result.content[:200].replace('\n', ' ')
            if len(result.content) > 200:
                content_preview += "..."
            output += f"   {content_preview}\n"

        return output

    except ValueError as e:
        # ValueError é€šå¸¸æ˜¯æˆ‘ä»¬è‡ªå·±æŠ›å‡ºçš„å‹å¥½é”™è¯¯
        return str(e)
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"[ERROR] æ£€ç´¢å¤±è´¥: {error_detail}")
        return f"âŒ æ£€ç´¢å¤±è´¥ï¼š{str(e)}"


@mcp.tool()
async def rag_answer(
    question: str,
    collection_name: Optional[str],
    num_variants: int = 3,
    k_per_query: int = 3,
    top_k: int = 10,
    top_n_synthesis: int = 5,
    ctx: Context = None
) -> str:
    """
    å®Œæ•´çš„ RAG æµç¨‹ï¼šæ”¹å†™æŸ¥è¯¢ã€æ£€ç´¢æ–‡æ¡£ã€é‡æ’åºå’Œåˆæˆç­”æ¡ˆã€‚

    è¿™æ˜¯æ‰§è¡Œå®Œæ•´å¤šç­–ç•¥æ£€ç´¢ä¼˜åŒ–çš„ä¸»è¦ RAG å·¥å…·ã€‚

    å‚æ•°ï¼š
        question: ç”¨æˆ·çš„é—®é¢˜
        collection_name: å¯é€‰çš„é›†åˆåç§°ï¼ˆå¦‚æœæœªå…¨å±€è®¾ç½®ï¼‰
        num_variants: è¦ç”Ÿæˆçš„æŸ¥è¯¢å˜ä½“æ•°é‡ï¼ˆ3-5ä¸ªï¼‰
        k_per_query: æ¯ä¸ªæŸ¥è¯¢å˜ä½“è¦æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
        top_k: é‡æ’åºåä¿ç•™çš„é¡¶éƒ¨æ–‡æ¡£æ•°é‡
        top_n_synthesis: ç”¨äºç­”æ¡ˆåˆæˆçš„é¡¶éƒ¨æ–‡æ¡£æ•°é‡
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        # å¦‚æœæä¾›äº†é›†åˆåç§°ï¼Œåˆ™è®¾ç½®é›†åˆ
        if collection_name:
            connector.set_collection(collection_name)
        elif not connector.vector_store:
            # å¦‚æœæ²¡æœ‰æä¾› collection_name ä¸” vector_store æœªåˆå§‹åŒ–ï¼Œè¿”å›å‹å¥½æç¤º
            return (
                "âŒ RAG æµç¨‹å¤±è´¥ï¼šå‘é‡å­˜å‚¨æœªåˆå§‹åŒ–\n\n"
                "è¯·å…ˆæ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š\n"
                "1. è°ƒç”¨ rag_set_collection è®¾ç½®é›†åˆï¼š\n"
                "   rag_set_collection(collection_name='your_collection_name')\n\n"
                "2. æˆ–è€…åœ¨è°ƒç”¨ rag_answer æ—¶æä¾› collection_name å‚æ•°ï¼š\n"
                "   rag_answer(question='...', collection_name='your_collection_name')\n\n"
                f"å½“å‰çŠ¶æ€ï¼š\n"
                f"  - Milvus URI: {connector.milvus_uri}\n"
                f"  - é›†åˆåç§°: {connector.milvus_collection or 'æœªè®¾ç½®'}"
            )

        output = f"ğŸš€ RAG æµç¨‹å¯åŠ¨\n"
        output += f"{'='*80}\n\n"
        output += f"ğŸ“¥ é—®é¢˜ï¼š{question}\n\n"

        # æ­¥éª¤ 1ï¼šæŸ¥è¯¢æ”¹å†™
        output += f"ğŸ“ æ­¥éª¤ 1ï¼šæŸ¥è¯¢æ”¹å†™\n"
        queries = await connector.rewrite_query(question, num_variants)
        output += f"   ç”Ÿæˆäº† {len(queries)} ä¸ªæŸ¥è¯¢å˜ä½“\n"
        for i, q in enumerate(queries, 1):
            output += f"   {i}. [{q['strategy']}] {q['rewritten']}\n"
        output += "\n"

        # æ­¥éª¤ 2ï¼šå¹¶è¡Œæ£€ç´¢
        output += f"ğŸ” æ­¥éª¤ 2ï¼šå¹¶è¡Œæ£€ç´¢\n"

        # åˆ›å»ºå¹¶è¡Œæ£€ç´¢ä»»åŠ¡
        async def retrieve_for_query(q: Dict[str, str]) -> tuple[str, List[RetrievalResult]]:
            """ä¸ºå•ä¸ªæŸ¥è¯¢æ‰§è¡Œæ£€ç´¢"""
            query_text = q.get("rewritten", q.get("original", ""))
            strategy = q.get("strategy", "unknown")
            results = await connector.retrieve_documents(
                query_text,
                k=k_per_query,
                strategy=strategy
            )
            return strategy, results

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ£€ç´¢ä»»åŠ¡
        retrieval_tasks = [retrieve_for_query(q) for q in queries]
        retrieval_results = await asyncio.gather(*retrieval_tasks)

        # æ”¶é›†æ‰€æœ‰ç»“æœ
        all_results = []
        for strategy, results in retrieval_results:
            all_results.extend(results)
            output += f"   [{strategy}] æ£€ç´¢åˆ° {len(results)} ä¸ªæ–‡æ¡£\n"

        output += f"   æ€»å…±æ£€ç´¢ï¼š{len(all_results)} ä¸ªæ–‡æ¡£\n\n"

        # æ­¥éª¤ 3ï¼šé‡æ’åº
        output += f"ğŸ¯ æ­¥éª¤ 3ï¼šé‡æ’åºå’Œå»é‡\n"
        reranked = await connector.rerank_results(all_results, question, top_k)
        output += f"   å»é‡åï¼š{len(reranked)} ä¸ªå”¯ä¸€æ–‡æ¡£\n"
        output += f"   é€‰æ‹© Top-{top_k} ç”¨äºåˆæˆ\n\n"

        # æ­¥éª¤ 4ï¼šç­”æ¡ˆåˆæˆ
        output += f"ğŸ“ æ­¥éª¤ 4ï¼šç­”æ¡ˆåˆæˆ\n"
        response = await connector.synthesize_answer(question, reranked, top_n_synthesis)
        output += f"   ä½¿ç”¨ top-{top_n_synthesis} ä¸ªæ–‡æ¡£\n"
        output += f"   å¹³å‡ç›¸å…³æ€§åˆ†æ•°ï¼š{response.metadata.get('avg_score', 0):.3f}\n\n"

        output += f"{'='*80}\n"
        output += f"ğŸ“¤ æœ€ç»ˆç­”æ¡ˆ\n"
        output += f"{'='*80}\n\n"
        output += response.answer
        output += f"\n\n{'='*80}\n"
        output += f"ğŸ“Š å…ƒæ•°æ®\n"
        output += f"{'='*80}\n"
        output += f"æ£€ç´¢åˆ°çš„æ–‡æ¡£æ€»æ•°ï¼š{len(all_results)}\n"
        output += f"å»é‡åçš„å”¯ä¸€æ–‡æ¡£ï¼š{len(reranked)}\n"
        output += f"ç”¨äºåˆæˆçš„æ–‡æ¡£ï¼š{response.metadata.get('top_n_used', 0)}\n"
        output += f"å”¯ä¸€æ¥æºæ•°ï¼š{response.metadata.get('num_sources', 0)}\n"
        output += f"å¹³å‡ç›¸å…³æ€§åˆ†æ•°ï¼š{response.metadata.get('avg_score', 0):.3f}\n\n"

        output += f"ğŸ“š ä¸»è¦æ¥æºï¼š\n"
        for i, source in enumerate(response.sources[:5], 1):
            output += f"{i}. {source['source']}ï¼ˆåˆ†æ•°ï¼š{source['score']:.3f}ï¼‰\n"

        return output

    except ValueError as e:
        # ValueError é€šå¸¸æ˜¯æˆ‘ä»¬è‡ªå·±æŠ›å‡ºçš„å‹å¥½é”™è¯¯
        return str(e)
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"[ERROR] RAG æµç¨‹å¤±è´¥: {error_detail}")
        return f"âŒ RAG æµç¨‹å¤±è´¥ï¼š{str(e)}"


@mcp.tool()
async def rag_multi_query_search(
    queries: List[str],
    collection_name: Optional[str] = None,
    k_per_query: int = 3,
    top_k: int = 10,
    ctx: Context = None
) -> str:
    """
    ä½¿ç”¨è‡ªå®šä¹‰æŸ¥è¯¢æ‰§è¡Œå¤šæŸ¥è¯¢æœç´¢ï¼ˆä¸è‡ªåŠ¨æ”¹å†™ï¼‰ã€‚

    å½“æ‚¨å·²ç»æœ‰å¤šä¸ªæŸ¥è¯¢è¡¨è¿°æ—¶å¾ˆæœ‰ç”¨ã€‚

    å‚æ•°ï¼š
        queries: æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨
        collection_name: å¯é€‰çš„é›†åˆåç§°
        k_per_query: æ¯ä¸ªæŸ¥è¯¢è¦æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
        top_k: å»é‡åè¿”å›çš„é¡¶éƒ¨æ–‡æ¡£æ•°é‡
    """
    connector = ctx.request_context.lifespan_context.connector

    try:
        # å¦‚æœæä¾›äº†é›†åˆåç§°ï¼Œåˆ™è®¾ç½®é›†åˆ
        if collection_name:
            connector.set_collection(collection_name)
        elif not connector.vector_store:
            # å¦‚æœæ²¡æœ‰æä¾› collection_name ä¸” vector_store æœªåˆå§‹åŒ–ï¼Œè¿”å›å‹å¥½æç¤º
            return (
                "âŒ å¤šæŸ¥è¯¢æœç´¢å¤±è´¥ï¼šå‘é‡å­˜å‚¨æœªåˆå§‹åŒ–\n\n"
                "è¯·å…ˆæ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š\n"
                "1. è°ƒç”¨ rag_set_collection è®¾ç½®é›†åˆï¼š\n"
                "   rag_set_collection(collection_name='your_collection_name')\n\n"
                "2. æˆ–è€…åœ¨è°ƒç”¨ rag_multi_query_search æ—¶æä¾› collection_name å‚æ•°ï¼š\n"
                "   rag_multi_query_search(queries=[...], collection_name='your_collection_name')\n\n"
                f"å½“å‰çŠ¶æ€ï¼š\n"
                f"  - Milvus URI: {connector.milvus_uri}\n"
                f"  - é›†åˆåç§°: {connector.milvus_collection or 'æœªè®¾ç½®'}"
            )

        output = f"å¤šæŸ¥è¯¢æœç´¢ï¼ˆ{len(queries)} ä¸ªæŸ¥è¯¢ï¼‰ï¼š\n\n"

        # åˆ›å»ºå¹¶è¡Œæ£€ç´¢ä»»åŠ¡
        async def retrieve_for_single_query(idx: int, query: str) -> tuple[int, str, List[RetrievalResult]]:
            """ä¸ºå•ä¸ªæŸ¥è¯¢æ‰§è¡Œæ£€ç´¢"""
            results = await connector.retrieve_documents(query, k=k_per_query)
            return idx, query, results

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ£€ç´¢ä»»åŠ¡
        retrieval_tasks = [retrieve_for_single_query(i, q) for i, q in enumerate(queries, 1)]
        retrieval_results = await asyncio.gather(*retrieval_tasks)

        # æ”¶é›†æ‰€æœ‰ç»“æœ
        all_results = []
        for idx, query, results in sorted(retrieval_results, key=lambda x: x[0]):
            all_results.extend(results)
            output += f"{idx}. æŸ¥è¯¢ï¼š{query}\n"
            output += f"   æ£€ç´¢åˆ°ï¼š{len(results)} ä¸ªæ–‡æ¡£\n\n"

        # é‡æ’åºå’Œå»é‡
        reranked = await connector.rerank_results(all_results, queries[0], top_k)

        output += f"{'='*80}\n"
        output += f"é‡æ’åºç»“æœï¼ˆTop-{top_k}ï¼‰ï¼š\n\n"

        for i, result in enumerate(reranked, 1):
            source = result.metadata.get('source', 'unknown')
            output += f"{i}. [åˆ†æ•°ï¼š{result.score:.3f}] {source}\n"
            content_preview = result.content[:150].replace('\n', ' ')
            if len(result.content) > 150:
                content_preview += "..."
            output += f"   {content_preview}\n\n"

        return output

    except ValueError as e:
        # ValueError é€šå¸¸æ˜¯æˆ‘ä»¬è‡ªå·±æŠ›å‡ºçš„å‹å¥½é”™è¯¯
        return str(e)
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"[ERROR] å¤šæŸ¥è¯¢æœç´¢å¤±è´¥: {error_detail}")
        return f"âŒ å¤šæŸ¥è¯¢æœç´¢å¤±è´¥ï¼š{str(e)}"


# ============================================================================
# ä¸»å…¥å£ç‚¹
# ============================================================================

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="RAG MCP æœåŠ¡å™¨")
    parser.add_argument(
        "--llm-provider", type=str, default="deepseek", help="LLM æä¾›å•†ï¼ˆä¾‹å¦‚ï¼šdeepseek, openaiï¼‰"
    )
    parser.add_argument(
        "--llm-model", type=str, default="deepseek-chat", help="LLM æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--llm-api-key", type=str, default="sk-0828827353434c24b51dd30edcfa7f32", help="LLM API å¯†é’¥"
    )
    parser.add_argument(
        "--embedding-model", type=str, default="qwen3-embedding:0.6b", help="åµŒå…¥æ¨¡å‹"
    )
    parser.add_argument(
        "--embedding-url", type=str, default="http://35.235.113.151:11434", help="åµŒå…¥æœåŠ¡ URL"
    )
    parser.add_argument(
        "--milvus-uri", type=str, default="http://121.40.159.60:19530", help="Milvus æœåŠ¡å™¨ URI"
    )
    parser.add_argument(
        "--milvus-collection", type=str, default=None, help="é»˜è®¤ Milvus é›†åˆ"
    )
    parser.add_argument("--sse", action="store_true", default=True, help="å¯ç”¨ SSE æ¨¡å¼")
    parser.add_argument("--port", type=int, default=8001, help="SSE æœåŠ¡å™¨ç«¯å£å·")
    return parser.parse_args()


def main():
    """ä¸»å…¥å£ç‚¹"""
    load_dotenv()
    args = parse_arguments()

    mcp.config = {
        "llm_provider": os.environ.get("LLM_PROVIDER", args.llm_provider),
        "llm_model": os.environ.get("LLM_MODEL", args.llm_model),
        "llm_api_key": os.environ.get("LLM_API_KEY", args.llm_api_key),
        "embedding_model": os.environ.get("EMBEDDING_MODEL", args.embedding_model),
        "embedding_base_url": os.environ.get("EMBEDDING_BASE_URL", args.embedding_url),
        "milvus_uri": os.environ.get("MILVUS_URI", args.milvus_uri),
        "milvus_collection": os.environ.get("MILVUS_COLLECTION", args.milvus_collection),
    }

    if args.sse:
        mcp.run(transport="sse", port=args.port, host="0.0.0.0")
    else:
        mcp.run()


if __name__ == "__main__":
    main()
